### week 8: sun694_w08_multilingual_translation.pdf: https://arxiv.org/abs/2002.02955

Summary: Using expectation maximization, you can train unsupervised machine translation models suprisingly well.

Empirical uses: SOTA in some tasks for unsupervised machine translation, provides a moral derivation of the backtranslation optimization.

### week 10: sun694_w10_BERT.pdf: https://arxiv.org/abs/1810.04805

Summary: Masked language modeling is able to produce extremely powerful models.

Empirical uses: SOTA in 11 tasks at time of publication. Still used today due to extreme flexability. Very good model.

### week 11: sun694_w11_MASS.pdf: https://arxiv.org/abs/1905.02450

Summary: Masked language modeling with explicit generative biases

Empirical uses: SOTA in unsupervised machine translation in many directions. Still used as pretraining today (mid 2020). Very good for what it does.

### week 12: sun694_w12_aesthetic_image_captioning.pdf: https://arxiv.org/abs/1908.11310

Summary: An im2text architecture for judging the artistic quality of a photo.

Empirical uses: Not many. However, the architecture they used was interesting, as was the method of gathering data.

### week 17: sun694_w17_large_memory_layers.pdf: https://arxiv.org/abs/1907.05242

Summary: Use a sparse clustering algorithm to generate memories

Empirical uses: Computationally efficient parameterization of models

## week 8: michael_w08_attn_is_all_you_need.pdf: https://arxiv.org/abs/1706.03762

Summary: Transformers - much more nerdy and less Michael Bay

Empirical uses: It does extremely well at seq-to-seq machine translation and some other stuff
